{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain langchain_community langchain_huggingface pypdf faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import gradio as gr\n",
    "import webview\n",
    "import threading\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica i due indici FAISS\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "        model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "\n",
    "\n",
    "vectorstore = FAISS.load_local(\"last_merge\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "\n",
    "\n",
    "# Definizione del prompt ottimizzato con tecniche avanzate\n",
    "template = \"\"\"\n",
    "<system>\n",
    "You are an AI assistant expert in Large Language Models (LLMs) and Natural Language Processing (NLP), designed to answer questions related exclusively to these topics.\n",
    "You work as the official chatbot of the University of Salerno (UNISA), Department of DIEM \n",
    "(Department of Information and Electrical Engineering and Applied Mathematics).\n",
    "You must answer both organizational questions about the course (such as the recommended book, professors, etc.) and questions regarding the topics covered.\n",
    "Answer only in english.\n",
    "</system>\n",
    "\n",
    "<instructions>\n",
    "1. Domain and Scope:\n",
    "   - Only answer questions related to LLMs and NLP.\n",
    "   - For any query outside this domain, respond humorously that you cannot help because you specialize exclusively in LLMs and NLP.\n",
    "2. Clear and Unified Responses:\n",
    "   - Provide a coherent answer that **systematically blends** information from the provided context with your **deep expertise in LLMs/NLP**.\n",
    "   - **Always enrich** the context with your internal knowledge to add value, technical depth, and didactic clarity, even when the context appears complete.\n",
    "   - Ensure the integration feels organic by:\n",
    "     • Using the context as foundation \n",
    "     • Expanding concepts with technical details from your knowledge\n",
    "     • Maintaining a natural flow between sources\n",
    "   - Structure the answer in logical paragraphs. \n",
    "   - At the end of the answer add \"---\".\n",
    "3. Contextual Integration:\n",
    "   - Use the conversation history and provided context to ensure continuity, relevance, and precision.\n",
    "   - If you find informations in other language, translate them in English.\n",
    "4. Name Verification and Precision:\n",
    "   - **Always check that any person's full name appears exactly as provided in the context**.\n",
    "   - **If a name is not found exactly, reply with: \"I have no information about this person. Perhaps you meant... ?\" and suggest the two closest alternatives**.\n",
    "5. Uncertainty and Transparency:\n",
    "   - If you are not 100% sure about any detail, admit the uncertainty rather than inventing information.\n",
    "6. Synonym and Variation Handling:\n",
    "   - Recognize that slight rephrasings (e.g., \"What is the book?\" vs. \"What is the recommended book for the course?, \"What is a LLM?\" vs \"What is a Large Language Model?\") refer to the same query.\n",
    "7. Code Provision Guidelines:\n",
    "   - Provide code only if it directly pertains to the information or functionalities present in the context.\n",
    "   - If the requested code falls outside the current context, state that you can only provide code for functionalities already covered.\n",
    "8. Defending the Answer:\n",
    "   - If challenged (e.g., \"Are you sure?\"), reaffirm that your answer is based on both the provided context and established knowledge of LLMs and NLP.\n",
    "   - Cite the relevant source if necessary.\n",
    "9. Handling Adversarial Questions:\n",
    "    - For questions that deviate from LLMs and NLP, respond humorously that you cannot help.\n",
    "</instructions>\n",
    "\n",
    "<examples>\n",
    "Question: \"What is a LLM?\"\n",
    "Answer: \"A Large Language Model (LLM) is a type of artificial intelligence (AI) model designed to understand, generate, and manipulate human language. These models are trained on vast amounts of text data and use advanced machine learning techniques, particularly deep learning, to perform tasks such as text generation, translation, summarization, question answering, and more. ---\"\n",
    "\n",
    "Question: \"What is NLP?\"\n",
    "Answer: \"Natural Language Processing (NLP) is a field of artificial intelligence (AI) that focuses on enabling machines to understand, interpret, generate, and manipulate human language. It combines linguistics and machine learning to process text and speech in a way that is both meaningful and useful. ---\"\n",
    "\n",
    "Question: \"What's the difference between skip-gram and continuous bag of words?\"\n",
    "Answer: \"Skip-gram predicts surrounding words given a target word, while Continuous Bag of Words (CBOW) predicts a target word from its surrounding context. Skip-gram tends to perform better on rare words by generating multiple training samples per word, whereas CBOW is faster and more efficient but may struggle with infrequent words. ---\"\n",
    "\n",
    "Question: \"Who is the president of the United States of America?\"\n",
    "Answer: \"Sorry, I have an exclusive relationship with the course of LLM and NLP. All other questions give me sudden selective amnesia! ---\"\n",
    "</examples>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<conversation history>\n",
    "{chat_history}\n",
    "</conversation history>\n",
    "\n",
    "\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<answer>\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"context\", \"chat_history\", \"question\"])\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antonio Russomando\\AppData\\Local\\Temp\\ipykernel_16756\\2256070349.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "# Creazione del retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 7}\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    input_key=\"question\",  \n",
    "    output_key=\"answer\"    \n",
    ")\n",
    "\n",
    "# Creazione della catena con memoria\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template},\n",
    "    return_source_documents=True,  \n",
    "    output_key=\"answer\"  \n",
    ")\n",
    "\n",
    "os.makedirs(\"valutazioni\", exist_ok=True)\n",
    "\n",
    "# Modifica della funzione save_evaluation\n",
    "def save_evaluation(evaluation_data):\n",
    "    \"\"\"Salva le valutazioni in un unico file JSON e calcola le medie\"\"\"\n",
    "    filename = \"valutazioni/valutazioni.json\"\n",
    "    \n",
    "    try:\n",
    "        # Leggi i dati esistenti o inizializza una nuova struttura\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {\n",
    "                \"valutazioni\": [],\n",
    "                \"media\": {\n",
    "                    \"pertinenza\": 0,\n",
    "                    \"coerenza\": 0,\n",
    "                    \"completezza\": 0,\n",
    "                    \"chiarezza\": 0,\n",
    "                    \"correttezza\": 0\n",
    "                }\n",
    "            }\n",
    "\n",
    "        # Aggiungi la nuova valutazione\n",
    "        data[\"valutazioni\"].append(evaluation_data)\n",
    "\n",
    "        # Calcola le medie solo dalle valutazioni valide\n",
    "        valutazioni_valide = [v for v in data[\"valutazioni\"] if \"error\" not in v]\n",
    "        \n",
    "        if valutazioni_valide:\n",
    "            # Calcola le somme per ogni criterio\n",
    "            somme = {\n",
    "                \"pertinenza\": 0,\n",
    "                \"coerenza\": 0,\n",
    "                \"completezza\": 0,\n",
    "                \"chiarezza\": 0,\n",
    "                \"correttezza\": 0\n",
    "            }\n",
    "\n",
    "            for valutazione in valutazioni_valide:\n",
    "                for criterio in somme:\n",
    "                    somme[criterio] += valutazione.get(criterio, 0)\n",
    "\n",
    "            # Calcola le medie\n",
    "            data[\"media\"] = {\n",
    "                criterio: round(somme[criterio] / len(valutazioni_valide), 2)\n",
    "                for criterio in somme\n",
    "            }\n",
    "\n",
    "        # Salva il file aggiornato\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nel salvataggio: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Modifica della funzione llm_judge\n",
    "def llm_judge(query, context, response):\n",
    "    evaluation_prompt = f\"\"\"\n",
    "    Valuta la risposta del chatbot come esperto NLP/LLM seguendo questi criteri (1-5):\n",
    "    1. Pertinenza alla domanda: \"{query}\"\n",
    "    2. Coerenza col contesto fornito: {context[:1000]}...\n",
    "    3. Completezza tecnica\n",
    "    4. Chiarezza espositiva\n",
    "    5. Correttezza accademica\n",
    "\n",
    "    Risposta da valutare: \"{response}\"\n",
    "\n",
    "    Fornisci i punteggi in formato JSON con questo schema:\n",
    "    {{\"pertinenza\": int, \"coerenza\": int, \"completezza\": int, \n",
    "    \"chiarezza\": int, \"correttezza\": int, \"commento\": \"stringa\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    judge_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "    evaluation = judge_llm.invoke(evaluation_prompt)\n",
    "    \n",
    "    try:\n",
    "        eval_data = json.loads(evaluation.content.replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n",
    "        eval_data.update({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query\": query,\n",
    "            \"response\": response\n",
    "        })\n",
    "        save_evaluation(eval_data)\n",
    "        return eval_data\n",
    "    except Exception as e:\n",
    "        error_data = {\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query\": query,\n",
    "            \"raw_response\": evaluation.content\n",
    "        }\n",
    "        save_evaluation(error_data)\n",
    "        return {\"error\": \"Errore nel parsing della valutazione\"}\n",
    "    \n",
    "\n",
    "def ask_question(query):\n",
    "    result = conversation_chain.invoke({\"question\": query})\n",
    "    response = result[\"answer\"]\n",
    "    \n",
    "    \n",
    "    context = \"\\n\".join([doc.page_content for doc in result[\"source_documents\"]])\n",
    "    \n",
    "    \n",
    "    evaluation = llm_judge(query, context, response)\n",
    "    \n",
    "    \n",
    "    print(f\"\\nValutazione Gemini: {evaluation}\\n\")\n",
    "    \n",
    "    if \"---\" in response:\n",
    "        response = response.split(\"---\")[0].strip()\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\gradio\\components\\chatbot.py:228: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\responses.py\", line 259, in __call__\n",
      "    await wrap(partial(self.listen_for_disconnect, receive))\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\responses.py\", line 255, in wrap\n",
      "    await func()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\responses.py\", line 232, in listen_for_disconnect\n",
      "    message = await receive()\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 534, in receive\n",
      "    await self.message_event.wait()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\asyncio\\locks.py\", line 212, in wait\n",
      "    await fut\n",
      "asyncio.exceptions.CancelledError: Cancelled by cancel scope 1c98071d250\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 406, in run_asgi\n",
      "  |     result = await app(  # type: ignore[func-returns-value]\n",
      "  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "  |     return await self.app(scope, receive, send)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "  |     await super().__call__(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "  |     await self.middleware_stack(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "  |     raise exc\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "  |     await self.app(scope, receive, _send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\gradio\\route_utils.py\", line 790, in __call__\n",
      "  |     await self.app(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "  |     raise exc\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "  |     await app(scope, receive, sender)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\routing.py\", line 715, in __call__\n",
      "  |     await self.middleware_stack(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\routing.py\", line 735, in app\n",
      "  |     await route.handle(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "  |     await self.app(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "  |     raise exc\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "  |     await app(scope, receive, sender)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\routing.py\", line 74, in app\n",
      "  |     await response(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\responses.py\", line 252, in __call__\n",
      "  |     async with anyio.create_task_group() as task_group:\n",
      "  |                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 763, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\responses.py\", line 255, in wrap\n",
      "    |     await func()\n",
      "    |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\responses.py\", line 244, in stream_response\n",
      "    |     async for chunk in self.body_iterator:\n",
      "    |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\gradio\\routes.py\", line 909, in iterator\n",
      "    |     done = [d.result() for d in done]\n",
      "    |             ^^^^^^^^^^\n",
      "    |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\gradio\\routes.py\", line 894, in stop_stream\n",
      "    |     await app.stop_event.wait()\n",
      "    |           ^^^^^^^^^^^^^^^^^^^\n",
      "    | AttributeError: 'NoneType' object has no attribute 'wait'\n",
      "    +------------------------------------\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\responses.py\", line 259, in __call__\n",
      "    await wrap(partial(self.listen_for_disconnect, receive))\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\responses.py\", line 255, in wrap\n",
      "    await func()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\responses.py\", line 232, in listen_for_disconnect\n",
      "    message = await receive()\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 534, in receive\n",
      "    await self.message_event.wait()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\asyncio\\locks.py\", line 212, in wait\n",
      "    await fut\n",
      "asyncio.exceptions.CancelledError: Cancelled by cancel scope 1c98071ed50\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 406, in run_asgi\n",
      "  |     result = await app(  # type: ignore[func-returns-value]\n",
      "  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "  |     return await self.app(scope, receive, send)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "  |     await super().__call__(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "  |     await self.middleware_stack(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "  |     raise exc\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "  |     await self.app(scope, receive, _send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\gradio\\route_utils.py\", line 790, in __call__\n",
      "  |     await self.app(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "  |     raise exc\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "  |     await app(scope, receive, sender)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\routing.py\", line 715, in __call__\n",
      "  |     await self.middleware_stack(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\routing.py\", line 735, in app\n",
      "  |     await route.handle(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "  |     await self.app(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "  |     raise exc\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "  |     await app(scope, receive, sender)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\routing.py\", line 74, in app\n",
      "  |     await response(scope, receive, send)\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\responses.py\", line 252, in __call__\n",
      "  |     async with anyio.create_task_group() as task_group:\n",
      "  |                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 763, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\responses.py\", line 255, in wrap\n",
      "    |     await func()\n",
      "    |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\starlette\\responses.py\", line 244, in stream_response\n",
      "    |     async for chunk in self.body_iterator:\n",
      "    |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\gradio\\routes.py\", line 909, in iterator\n",
      "    |     done = [d.result() for d in done]\n",
      "    |             ^^^^^^^^^^\n",
      "    |   File \"c:\\ProgramData\\anaconda3\\envs\\llm\\Lib\\site-packages\\gradio\\routes.py\", line 894, in stop_stream\n",
      "    |     await app.stop_event.wait()\n",
      "    |           ^^^^^^^^^^^^^^^^^^^\n",
      "    | AttributeError: 'NoneType' object has no attribute 'wait'\n",
      "    +------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valutazione Gemini: {'pertinenza': 5, 'coerenza': 5, 'completezza': 5, 'chiarezza': 5, 'correttezza': 5, 'commento': \"La risposta è eccellente. È pertinente, coerente con il contesto fornito, completa tecnicamente, chiara nell'esposizione e corretta dal punto di vista accademico. L'aggiunta di spiegazioni sui concetti chiave (come zero-shot learning, BPE, positional encoding) aumenta notevolmente il valore della risposta.\", 'timestamp': '2025-03-23T16:07:08.553143', 'query': 'Tell me the differences between LLama and GPT', 'response': \"The comparison between the LLaMA and GPT models can be broken down into several key aspects:\\n\\n**Size Range:** LLaMA models come in sizes of 7B, 13B, 30B, and 65B parameters. GPT models range from 117M to 175B+ parameters, including GPT-3. The size of a model often correlates with its capacity to learn and generalize from data; larger models can capture more complex patterns but require more computational resources.\\n\\n**Training Data:** LLaMA is trained on public data, including The Pile, Wikipedia, and Common Crawl. GPT is also trained on public data, including Common Crawl and WebText. The quality, diversity, and size of the training data significantly impact a model's performance and ability to generalize across different tasks and domains.\\n\\n**Performance:** LLaMA offers strong and competitive performance, especially for smaller models. GPT exhibits state-of-the-art performance, particularly in zero/few-shot learning scenarios. Zero-shot learning refers to the ability of a model to perform tasks without any specific training examples, while few-shot learning involves learning from only a small number of examples.\\n\\n**Training Efficiency:** LLaMA is more efficient and parameter-efficient in its training. GPT, especially GPT-3, is resource-intensive to train. Training efficiency is crucial for reducing the computational cost and time required to develop and deploy large language models.\\n\\n**Deployment:** LLaMA is open-sourced, providing flexible deployment options. GPT is deployed via a commercial API through OpenAI. Open-source models allow for greater customization and control, while commercial APIs offer ease of use and access to pre-trained models.\\n\\n**Ethics:** LLaMA incorporates strong ethical considerations. GPT has faced criticism regarding transparency and biases. Ethical considerations are increasingly important in the development and deployment of language models to ensure fairness, accountability, and transparency.\\n\\n**Applications:** LLaMA is suited for academic research and custom deployments. GPT finds broad commercial use through APIs and various applications. The choice of model depends on the specific use case, available resources, and desired level of customization.\\n\\nBoth LLaMA and GPT are pre-trained using an autoregressive language modeling objective, predicting the next token given previous tokens. LLaMA models use Byte-Pair Encoding (BPE) as their input encoding method, obtaining a dictionary of 32768 tokens and uses relative positional encodings instead of absolute positional encodings.\\n\\n---\"}\n",
      "\n",
      "\n",
      "Valutazione Gemini: {'pertinenza': 5, 'coerenza': 5, 'completezza': 4, 'chiarezza': 5, 'correttezza': 5, 'commento': 'La risposta è eccellente. È pertinente alla domanda, coerente con il contesto fornito, chiara, corretta e sufficientemente completa. Avrebbe potuto menzionare GPT-4 per una completezza ancora maggiore, ma la sua assenza non inficia significativamente la qualità della risposta.', 'timestamp': '2025-03-23T16:07:28.077374', 'query': 'Talk me about GPT', 'response': 'GPT (Generative Pre-trained Transformer) is a family of decoder-only transformer models developed by OpenAI. These models are designed to generate human-like text by learning to understand and predict language patterns from massive text datasets, enabling them to perform various natural language tasks without task-specific training.\\n\\nHere are some key GPT models:\\n\\n*   **GPT-2 (2019):** This model has an XL version containing 1.5 billion parameters, which includes 48 decoder blocks, 1600-dimensional embeddings, and 25 attention heads per block. GPT-2 is known for generating coherent, long-form text.\\n*   **GPT-3 (2020):** A significantly larger model with 175 billion parameters, consisting of 96 decoder blocks, 12,288-dimensional embeddings, and 96 attention heads per block. GPT-3 demonstrates advanced capabilities in language understanding, code generation, and even rudimentary reasoning.\\n\\nGPT models excel in various applications, including programming help through code generation and debugging, as well as summarization by generating concise summaries of long documents.\\n\\n---'}\n",
      "\n",
      "\n",
      "Valutazione Gemini: {'pertinenza': 5, 'coerenza': 5, 'completezza': 5, 'chiarezza': 5, 'correttezza': 5, 'commento': 'La risposta è eccellente. Definisce chiaramente TF-IDF, spiegando i componenti TF e IDF, il loro significato e come vengono combinati. Inoltre, menziona le applicazioni e le limitazioni, in linea con il contesto fornito. La spiegazione è chiara, completa e tecnicamente corretta.', 'timestamp': '2025-03-23T16:07:47.472995', 'query': 'What is TD-IDF?', 'response': 'TF-IDF (Term Frequency - Inverse Document Frequency) is a numerical statistic used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It is calculated as the product of two main components: Term Frequency (TF) and Inverse Document Frequency (IDF).\\n\\n**Term Frequency (TF):** This measures how frequently a term occurs in a document. It emphasizes words that appear more often in a specific document.\\n\\n**Inverse Document Frequency (IDF):** This measures the uniqueness of a word across the entire corpus. It emphasizes infrequent words, giving them higher weight because they are more likely to be significant for distinguishing documents. Common words that appear frequently in many documents have lower IDF values because they contribute less to differentiating between documents.\\n\\nTF-IDF is calculated by multiplying the TF and IDF values. A high TF-IDF score indicates that a term is frequent in a particular document and rare in the corpus, making it highly relevant to that document. Conversely, a low TF-IDF score suggests that the term is either infrequent in the document or common across the corpus, reducing its significance.\\n\\nDespite its limitations, TF-IDF is suitable for various applications, including information retrieval, information filtering, and text classification. However, more sophisticated techniques are needed for tasks such as text generation, automatic translation, question answering, and paraphrasing.\\n\\n---'}\n",
      "\n",
      "\n",
      "Valutazione Gemini: {'pertinenza': 5, 'coerenza': 5, 'completezza': 4, 'chiarezza': 5, 'correttezza': 5, 'commento': \"La risposta è eccellente. Definisce l'attention mechanism, lo contestualizza in NLP/LLM, lo lega al self-attention e al Transformer, spiega il funzionamento con Q, K, V e accenna all'Encoder-Decoder Attention. La chiarezza espositiva è ottima e la risposta è coerente con il contesto fornito. Unico miglioramento possibile sarebbe un esempio più concreto di come vengono calcolate le matrici Q, K, V, ma la risposta è già molto buona.\", 'timestamp': '2025-03-23T16:08:06.323256', 'query': 'What is the attention mechanism?', 'response': 'The attention mechanism is a crucial component in modern neural networks, especially in the context of Natural Language Processing (NLP) and Large Language Models (LLMs). It allows the model to focus on the most relevant parts of the input sequence when processing it, mimicking how humans pay attention to different aspects of information.\\n\\nOne of the primary examples of the attention mechanism is the self-attention mechanism, which is a core part of the Transformer architecture. Self-attention enables the model to weigh the importance of different words in a sentence relative to each other. For example, in the sentence \"The animal didn’t cross the street because it was too wide,\" the meaning of \"it\" depends heavily on the context. Self-attention identifies which words are most relevant to understanding the meaning of each word, learning these relationships during training.\\n\\nThe attention function computes the relevance of different input elements when encoding a particular word. It operates based on three matrices: Query (Q), Key (K), and Value (V). These matrices are derived through linear transformations of the input embeddings. The attention function is defined as a weighted sum of values, where the weights are determined by the similarity between the query and keys using a scaled dot-product. This similarity is calculated as the dot product of the query and key vectors, scaled by the square root of the key vector\\'s dimension to prevent the dot product from becoming excessively large. A softmax function is then applied to these scaled dot products to obtain attention weights, which are probabilities summing to 1. Finally, these attention weights are used to compute a weighted sum of the value vectors. This process is differentiable, efficient, and easily parallelizable, enabling efficient training using backpropagation.\\n\\nThe attention mechanism is also used in \"Encoder-Decoder Attention,\" which is a module or component within a larger system, possibly related to sequence-to-sequence models or attention mechanisms in neural networks.\\n\\n---'}\n",
      "\n",
      "\n",
      "Valutazione Gemini: {'pertinenza': 5, 'coerenza': 3, 'completezza': 4, 'chiarezza': 4, 'correttezza': 5, 'commento': 'La risposta è pertinente alla domanda, fornendo codice per il fine-tuning di un modello in Python. La coerenza è parzialmente compromessa perché il contesto fornito riguarda un grafico di accuratezza e codice per la generazione di testo, mentre la risposta si concentra sul fine-tuning con la libreria `trl`. La risposta è completa nel fornire esempi di codice per SFTTrainer, RewardTrainer e PPOTrainer, con spiegazioni chiare. Il codice sembra corretto e utilizza una libreria appropriata per il fine-tuning di modelli di linguaggio.', 'timestamp': '2025-03-23T16:08:52.555983', 'query': 'Provide me a code for fine-tune a model in python', 'response': 'Certainly! Here\\'s the code for fine-tuning a model in Python, based on the provided context, along with explanations to enhance your understanding.\\n\\nThe description outlines a multi-step process for fine-tuning large language models using the `trl` library, encompassing Supervised Fine-Tuning (SFT), Reward Modeling, and Proximal Policy Optimization (PPO).\\n\\n**Step 1: SFTTrainer**\\n\\nThis step involves training the model on a custom dataset.\\n\\n**Code Description:**\\n\\nThis code snippet demonstrates how to use the `SFTTrainer` from the `trl` library to fine-tune a language model using supervised learning. It initializes the `SFTTrainer` with a pre-trained model, a training dataset, and specifies the text field to be used for training, along with the maximum sequence length. Finally, it calls the `train()` method to start the fine-tuning process.\\n\\n**Code Snippet:**\\n\\n```python\\nfrom trl import SFTTrainer\\n\\ntrainer = SFTTrainer(\\n    model_id=\"facebook/opt-350m\",\\n    train_dataset=dataset,\\n    dataset_text_field=\"text\",\\n    max_seq_length=512,\\n)\\ntrainer.train()\\n```\\n\\n**Step 2: RewardTrainer**\\n\\nThis step involves training a reward model.\\n\\n**Code Description:**\\n\\nThis code snippet demonstrates how to use the `RewardTrainer` from the `trl` library to train a reward model. It initializes the `RewardTrainer` with a model, training arguments, a tokenizer, and a training dataset. The `train()` method is then called to initiate the training process.\\n\\n**Code Snippet:**\\n\\n```python\\nfrom trl import RewardTrainer\\n\\ntrainer = RewardTrainer(\\n    model=model,\\n    args=training_args,\\n    tokenizer=tokenizer,\\n    train_dataset=dataset,\\n)\\ntrainer.train()\\n```\\n\\n**Step 3: PPOTrainer**\\n\\nThis step focuses on further optimization using the rewards obtained from the reward model and the PPO algorithm.\\n\\n**Code Description:**\\n\\nThis code snippet demonstrates how to use the `PPOTrainer` from the `trl` library to optimize a language model using Proximal Policy Optimization (PPO). It initializes the `PPOTrainer` with a configuration, a model, and a tokenizer. The code then iterates through a data loader, generates responses from the model based on queries, calculates rewards using a reward model, and performs a training step using the `trainer.step()` method, passing in the query, response, and reward.\\n\\n**Code Snippet:**\\n\\n```python\\nfrom trl import PPOConfig, PPOTrainer\\n\\ntrainer = PPOTrainer(config, model, tokenizer)\\n\\nfor query in dataloader:\\n    response = model.generate(query)\\n    reward = reward_model(response)\\n    trainer.step(query, response, reward)\\n```\\n\\nThese snippets use the `trl` library, which simplifies the process of fine-tuning models using techniques like Supervised Fine-Tuning (SFT), Reward Modeling, and Proximal Policy Optimization (PPO). Each trainer is designed for a specific stage in the fine-tuning process, allowing for modular and efficient optimization.\\n\\n---'}\n",
      "\n",
      "\n",
      "Valutazione Gemini: {'pertinenza': 5, 'coerenza': 5, 'completezza': 4, 'chiarezza': 5, 'correttezza': 5, 'commento': \"La risposta è pertinente, coerente con il contesto fornito (ripete la definizione di RLHF), chiara e corretta. La completezza è buona, anche se si potrebbe approfondire ulteriormente le differenze pratiche e i casi d'uso specifici di ciascuna tecnica. Ottima la sintesi finale.\", 'timestamp': '2025-03-23T16:09:33.199247', 'query': \"What is the difference between RFHL and Fine-Tuning'\", 'response': \"RLHF (Reinforcement Learning from Human Feedback) and Fine-Tuning are both techniques used to improve Large Language Models (LLMs), but they approach the task from different angles and serve distinct purposes.\\n\\n**RLHF (Reinforcement Learning from Human Feedback):** RLHF is an advanced technique that uses human feedback to refine and improve the performance of Large Language Models. Instead of relying solely on pre-existing training data, RLHF integrates human evaluation and guidance into the learning process. This allows the model to be oriented towards more desirable behaviors, aligning its responses with human preferences and values. RLHF aims to balance the model's high performance capabilities with its ethical compliance and user expectations.\\n\\n**Fine-Tuning:** Fine-tuning refers to adapting a pre-trained LLM to a specific task by training it further on a task-specific dataset. This process involves using gradient descent on the model's weights to optimize performance on a specific task. The options for what to fine-tune include the full network, readout heads, and adapters. The latter two, readout heads and adapters, are collectively referred to as parameter-efficient fine-tuning. Fine-tuning changes the model itself. Fine-tuning is useful to Specialize LLMs for domain-specific tasks and to Improve accuracy and relevance for specific applications.\\n\\nIn summary, while both RLHF and fine-tuning aim to improve LLMs, they do so through different mechanisms. Fine-tuning uses task-specific data to adjust the model's parameters directly, whereas RLHF incorporates human feedback to guide the model towards more desirable and aligned behaviors.\\n\\n---\"}\n",
      "\n",
      "\n",
      "Valutazione Gemini: {'pertinenza': 5, 'coerenza': 5, 'completezza': 5, 'chiarezza': 5, 'correttezza': 5, 'commento': 'La risposta è perfettamente pertinente, coerente, completa, chiara e corretta. Estrae le informazioni rilevanti dal contesto fornito e le presenta in modo conciso e utile.', 'timestamp': '2025-03-23T16:09:55.511628', 'query': 'Do you know Antonio Greco?', 'response': 'Yes, **Antonio Greco** is one of the instructors for the course at the DIEM – University of Salerno. You can reach **Antonio Greco** at DIEM, FSTEC-05P01036, agreco@unisa.it, or by phone at 089 963003.\\n---'}\n",
      "\n",
      "\n",
      "Valutazione Gemini: {'pertinenza': 5, 'coerenza': 5, 'completezza': 4, 'chiarezza': 5, 'correttezza': 5, 'commento': \"La risposta è perfettamente pertinente, coerente con il contesto fornito e chiara. Identifica correttamente l'altro istruttore e fornisce le informazioni di contatto. Avrebbe potuto menzionare anche Antonio Greco come istruttore, ma la domanda chiedeva *l'altro* istruttore, quindi la risposta è comunque corretta.\", 'timestamp': '2025-03-23T16:10:16.465860', 'query': 'Do you know the other instructor of the course?', 'response': 'Yes, the other instructor of the course is **Nicola Capuano**. You can contact **Nicola Capuano** at DIEM, FSTEC-05P02007, ncapuano@unisa.it, or by phone at 089 964292.\\n---'}\n",
      "\n",
      "\n",
      "Valutazione Gemini: {'pertinenza': 1, 'coerenza': 1, 'completezza': 1, 'chiarezza': 5, 'correttezza': 1, 'commento': \"The response is completely irrelevant to the provided context. The context is about customer service interactions and sentiment analysis, not about the chatbot's role or department. The chatbot hallucinates a role at the University of Salerno, which is not related to the given data. The clarity of the response is high, but the content is entirely fabricated and unrelated.\", 'timestamp': '2025-03-23T16:10:36.490214', 'query': 'Which department do you serve?', 'response': 'I serve as the official chatbot for the University of Salerno (UNISA), specifically within the Department of DIEM (Department of Information and Electrical Engineering and Applied Mathematics). My expertise is focused exclusively on Large Language Models (LLMs) and Natural Language Processing (NLP). So, if you have any questions about these topics, feel free to ask!\\n---'}\n",
      "\n",
      "\n",
      "Valutazione Gemini: {'pertinenza': 5, 'coerenza': 5, 'completezza': 5, 'chiarezza': 5, 'correttezza': 5, 'commento': 'La risposta è perfettamente pertinente, coerente con il contesto fornito, completa, chiara e corretta. Estrae correttamente i nomi dei membri del gruppo dal testo.', 'timestamp': '2025-03-23T16:11:34.079865', 'query': 'Who are the members of the group?', 'response': 'The members of the project group are:\\n*   **Antonio Russomando**, matricola 0622702407, a.russomando12@studenti.unisa.it\\n*   **Luca Memoli**, matricola 0622702309, l.memoli14@studenti.unisa.it\\n*   **Raffaele Solimeno**, matricola 0622702308, r.solimeno2@studenti.unisa.it\\n*   **Giuseppe Marotta**, matricola 0622702302, g.marotta31@studenti.unisa.it\\n\\n---'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funzione per gestire la chat - modificata per supportare la cronologia delle conversazioni\n",
    "def gradio_chat(message, history):\n",
    "    risposta = ask_question(message)\n",
    "    return risposta\n",
    "\n",
    "# Creazione dell'interfaccia ChatInterface di Gradio\n",
    "chat_interface = gr.ChatInterface(\n",
    "    fn=gradio_chat,\n",
    "    title=\"Chatbot Team 4\",\n",
    "    description=\"By Raffaele Solimeno, Antonio Russomando, Luca Memoli and Giuseppe Marotta\",\n",
    "    examples=[\"What is a Large Language Model?\", \"What is Natural Language Processing?\", \"Who are the members of the project group?\"],\n",
    "    theme=\"JohnSmith9982/small_and_pretty\"\n",
    ")\n",
    "\n",
    "# Funzione per avviare Gradio in un thread separato\n",
    "def start_gradio():\n",
    "    chat_interface.launch(server_name=\"127.0.0.1\", server_port=7861, share=False, inbrowser=False)\n",
    "\n",
    "# Avvio di Gradio in un thread separato\n",
    "threading.Thread(target=start_gradio, daemon=True).start()\n",
    "\n",
    "# Attendi qualche istante per il lancio del server e poi apri una nuova finestra desktop con pywebview\n",
    "webview.create_window(\"Chatbot LLM e NLP\", \"http://127.0.0.1:7861\", width=1200, height=700)\n",
    "webview.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
